{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g4yPFpOI_eb",
        "outputId": "b74e2570-8392-460b-8686-47679314bfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqr69kmFJALD"
      },
      "source": [
        "**1. Preprocessing and Document Indexing (Retriever Techniques):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilqML-MDI1zM",
        "outputId": "67639ca7-4340-469a-95f4-b152a3e6f621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfLf-d0fI5zM",
        "outputId": "de5bd6d8-d670-4c34-fd40-315ff31da8ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "# Import necessary libraries\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "\n",
        "\n",
        "# Load the document\n",
        "with open('Employee_info.txt', 'r') as file:\n",
        "    documents = file.readlines()\n",
        "\n",
        "# Initialize the tokenizer and model for document encoding\n",
        "tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "model = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Tokenize and encode the documents\n",
        "inputs = tokenizer(documents, return_tensors='pt', padding=True, truncation=True)\n",
        "embeddings = model(**inputs).pooler_output\n",
        "\n",
        "# Index the documents using Faiss\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings.detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlYNkwmSJ1W-"
      },
      "source": [
        "**2. User Query and Document Retrieval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHcmUtHsJw0W",
        "outputId": "cd30b0fb-f344-4ac9-face-5855eca58371"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "\n",
        "# Initialize the tokenizer and model for question encoding\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "question_model = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "\n",
        "def get_user_query():\n",
        "    \"\"\"\n",
        "    Prompts the user to enter a query in a text box using a loop to handle potential errors.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        query = input(\"Enter your query about employees (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "        return query\n",
        "\n",
        "def retrieve_documents(query):\n",
        "    \"\"\"\n",
        "    Encodes the user's query and retrieves relevant documents from the Faiss index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of retrieved document strings.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode the query\n",
        "    question_inputs = question_tokenizer(query, return_tensors='pt')\n",
        "    question_embedding = question_model(**question_inputs).pooler_output\n",
        "\n",
        "    # Retrieve the top-k relevant documents\n",
        "    k = 3\n",
        "    D, I = index.search(question_embedding.detach().numpy(), k)\n",
        "    retrieved_docs = [documents[i] for i in I[0]]\n",
        "    return retrieved_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2vzpA9bKCKn"
      },
      "source": [
        "**3. Context Generation and Response with LLM (Integrating LLMs with Retrieved Information):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d89Ri2jtNu_s",
        "outputId": "4ebcb9a1-da74-4f20-b669-502f5e5d3dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu transformers openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUpI0qfRJ_UY",
        "outputId": "21db53c9-8bc5-42f2-82de-4702047a2376"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query about employees (or 'quit' to exit): How many employees are located in Bangalore?\n",
            "Based on the information provided:\n",
            "\n",
            "1. Bangalore has 4 employees.\n",
            "2. Chennai has the highest number of employees, with a total of 8.\n",
            "3. The total salary of all employees located in Pune is 4,210,000.\n",
            "\n",
            "If we assume that each employee in each location earns an equal salary, we can calculate the average salary for employees in each location by dividing the total salary by the number of employees:\n",
            "\n",
            "1. Bangalore:\n",
            "Total Employees: 4\n",
            "Total Salary: Unknown\n",
            "Average Salary per Employee: Unknown\n",
            "\n",
            "2. Chennai:\n",
            "Total Employees: 8\n",
            "Total Salary: Unknown\n",
            "Average Salary per Employee: Unknown\n",
            "\n",
            "3. Pune:\n",
            "Total Employees: Unknown\n",
            "Total Salary: 4,210,000\n",
            "Average Salary per Employee: Unknown\n",
            "\n",
            "Without precise salary information for each location, we are unable to provide specific salary figures or detailed summaries for each location.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "def retrieve_and_respond(query):\n",
        "    \"\"\"\n",
        "    Retrieves relevant documents, combines them into context, and generates a response using GPT-3.5-turbo.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's query.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response from GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "\n",
        "    retrieved_docs = retrieve_documents(query)\n",
        "\n",
        "    # Combine retrieved documents into a single context\n",
        "    context = \" \".join(retrieved_docs)\n",
        "\n",
        "    # Use OpenAI API key (replace with your actual key)\n",
        "    #api_key = ''  # Replace with your OpenAI API key\n",
        "\n",
        "    # Generate a response using GPT-3.5-turbo\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Based on the following information: {context}. Please provide a detailed summary.\"}\n",
        "            ]\n",
        "        )\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except openai.error.OpenAIError as error:\n",
        "        print(f\"OpenAI API Error: {error}\")\n",
        "        return \"An error occurred while generating the response. Please try again later.\"\n",
        "\n",
        "# Main loop\n",
        "while True:\n",
        "    query = get_user_query()\n",
        "    if query.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    response = retrieve_and_respond(query)\n",
        "    print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk1WwKIJKgku"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
